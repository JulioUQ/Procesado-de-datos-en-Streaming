{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b86be79",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d2dee6545b27d4d4733e88b92b1c74dd",
     "grade": false,
     "grade_id": "cell-c37c14bf24e0ec8d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Actividad 2: Streaming - adquisición y análisis de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c499d2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1badd939c73525194f40f645f3c156ce",
     "grade": false,
     "grade_id": "cell-758412ed8ae90120",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Introducción\n",
    "\n",
    "En esta actividad, exploraremos dos de las tecnologías más populares utilizadas para la adquisición y análisis de datos en tiempo real: Apache Kafka y Apache Spark Streaming. A través de una serie de ejercicios, aprenderéis los conceptos básicos de estas tecnologías y cómo usarlas para procesar datos en tiempo real.\n",
    "\n",
    "## Estructura\n",
    "\n",
    "Hemos dividido esta actividad en tres partes:\n",
    "\n",
    "1.  **Introducción a Apache Kafka:** En esta primera parte, aprenderéis los conceptos básicos de Apache Kafka, \n",
    "    sus principales componentes y cómo interactuar con ellos utilizando la interfaz de línea de comandos.\n",
    "2.  **Ingesta de datos con Apache Kafka:** En esta segunda parte, aprenderéis a utilizar Apache Kafka para ingerir\n",
    "    datos desde un productor y consumirlos desde un consumidor.\n",
    "3.  **Procesamiento de datos en tiempo real con Apache Spark Streaming:** En esta tercera parte, aprenderéis a usar \n",
    "    Apache Spark Streaming para procesar datos en tiempo real provenientes de Apache Kafka.\n",
    "    \n",
    "\n",
    "## Notas importantes:\n",
    "\n",
    "- La actividad debe realizarse en **grupos de 2 miembros**. \n",
    "  Asegúrate de saber quién es tu compañero antes de comenzar la actividad.\n",
    "- Aunque es posible completar las actividades directamente en este cuaderno, **desaconsejamos hacerlo** debido a posibles problemas de rendimiento del servidor. Verás que cada actividad está   contenida en su propia celda, lo que te permitirá copiarla fácilmente a un archivo Python. Este archivo puede ser **ejecutado en el servidor utilizando el terminal de Jupyterlab, SSH o VSCode**. Una vez que hayas ejecutado y probado el script con éxito,   simplemente cópialo de nuevo en la celda correspondiente del cuaderno. Este enfoque garantiza una ejecución más fluida y una mejor gestión de los recursos del servidor.\n",
    "- En algunos ejercicios, necesitarás tomar **capturas de pantalla para justificar tus respuestas**. \n",
    "  Puedes capturar imágenes utilizando las herramientas del sistema operativo que estés usando: \"Recortes\" en         Windows, \"Imprimir pantalla\",\\\" `Ctrl+C` al seleccionar una imagen, etc. \n",
    "  Una vez capturadas, puedes pegar las imágenes directamente en las celdas de respuesta usando `Ctrl+V` o el menú     contextual que aparece al hacer clic derecho, que permitirá pegar la imagen del portapapeles. \n",
    "  Para visualizar la imagen, debes ejecutar la celda.\n",
    "- **Debes utilizar únicamente las bibliotecas proporcionadas, a menos que se indique lo contrario.**\n",
    "- Por favor, no cambies el nombre del cuaderno ni el tipo de las celdas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb08530",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a5c5949078d6e6469c6ee986dfa48f6",
     "grade": false,
     "grade_id": "cell-d293ed241598a70c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Parte I: Introducción a Apache Kafka (2 puntos)\n",
    "\n",
    "[Kafka](https://kafka.apache.org) es una plataforma distribuida para gestionar \n",
    "eventos en streaming que nos permite leer, escribir y procesar eventos \n",
    "(registros o mensajes, según la terminología de Kafka) distribuidos a lo largo de un clúster.\n",
    "\n",
    "Comenzaremos la actividad creando un tema llamado `activity2<usuario>`\n",
    "en el servicio de Kafka de nuestro clúster (sustituye `<usuario>` por tu nombre de usuario). \n",
    "Un tema es una colección ordenada de eventos que se almacena de forma persistente, generalmente en disco,\n",
    "y se distribuye y replica. Kafka trata cada tema en cada partición como un registro \n",
    "(un conjunto ordenado de mensajes). Cada mensaje en una partición tiene un desplazamiento único, \n",
    "y estos mensajes tienen un período de retención predeterminado de 7 días (604,800,000 ms), \n",
    "aunque puedes modificarlo en el momento de la creación del tema.\n",
    "\n",
    "El broker de Kafka es `eimtcld3node1`, accesible en el puerto estándar 9092."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1650335e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "10ad0e30c8b84f1a48d63b2c88ee498b",
     "grade": false,
     "grade_id": "cell-7cad57cad27779ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Ejercicio 1: Crear un tema con Kafka (0.25 puntos)\n",
    "\n",
    "Crea un tema de Kafka llamado `activity2<usuario>` en nuestro clúster con un \n",
    "factor de replicación de 1 y una única partición, lo que significa que usaremos \n",
    "un solo nodo para almacenar los mensajes que recibe Kafka. \n",
    "Además, especifica que los mensajes solo se almacenen durante 2 horas en el tema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79647b0c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2d50ddf9aa1acd51a2f52a42c93f50d",
     "grade": true,
     "grade_id": "cell-ec6b56255dffcc3c",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23e84b8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8c6e5e35aa396be89628783afced21ef",
     "grade": false,
     "grade_id": "cell-6c2e29722b9cd99b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Ejercicio 2: Lista los temas de Kafka (0.25 puntos)\n",
    "\n",
    "Consulta el *tema* que acabas de crear y muéstralo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c62a0a4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe96623764634c99675a6ea9f695c9d2",
     "grade": true,
     "grade_id": "cell-b05732c7c474ceb8",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1c6bd7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bb74568ebcdfe6673cd99d79118777a6",
     "grade": false,
     "grade_id": "cell-d5d24f1092688463",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Ejercicio 3: Borra el tema de Kafka (0.25 puntos)\n",
    "\n",
    "Borra el *tema* que creaste en el Ejercicio 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c357a3c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "00823c545d6787273c0b04c7efa2d21f",
     "grade": true,
     "grade_id": "cell-0fabd287d5e83b34",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ae0639",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "62b8247751447e2a04b3d684606f6dbc",
     "grade": false,
     "grade_id": "cell-316cdff2cc7e17df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Ejercicio 4: Describe el tema de Kafka (0.25 puntos)\n",
    "\n",
    "Crea el *tema* de nuevo tal y como hiciste en el Ejercicio 1 y utiliza la línea de comandos de kafka para decribirlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc816b6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5226e2a1a309282405195947f8cb205",
     "grade": true,
     "grade_id": "cell-62b105cd150b41b5",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fca2272",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8d52f48aececc266b8390124abee52d5",
     "grade": false,
     "grade_id": "cell-c83b99fd0989f458",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Ejercicio 5: Crea un productor en Kafka (0.5 puntos)\n",
    "\n",
    "Vamos a crear un evento en el tema. Recuerda que este comando debe ejecutarse\n",
    "desde la terminal para interactuar. Recuerda usar `CTRL+c` cuando hayas terminado de enviar los mensajes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe183ae1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc830f39d29f559d5b2cca4b06588527",
     "grade": true,
     "grade_id": "cell-86aa21dca9ad2af3",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1574e8d2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2b58e8a2a9d95dd11a3a3ae8a34d21db",
     "grade": false,
     "grade_id": "cell-053c2241e4c3f039",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Ejercicio 6: Crea un consumidor en Kafka (0.5 puntos)\n",
    "\n",
    "Finalmente, se te pide consultar los mensajes enviados a través de la terminal \n",
    "utilizando el consumidor incorporado de Kafka para el tema. Específicamente, debes ejecutar \n",
    "un consumidor conectándote a los distintos brokers existentes y especificando el tema \n",
    "y la partición a los que se han enviado los mensajes. Puedes abrir dos terminales \n",
    "y verificar que los mensajes enviados con el productor al broker pueden ser \n",
    "visualizados con el consumidor de consola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e4ceb4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b64a5d9bcb059f236624f5e8073b13c9",
     "grade": true,
     "grade_id": "cell-097f3fa710421948",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a3d1ba",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c1f1e7c6caae3f5cec81136c29109b74",
     "grade": false,
     "grade_id": "cell-4e3eb51a468a7e25",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Parte II: Ingesta de datos con Apache Kafka (1.5 puntos)\n",
    "\n",
    "Para automatizar la generación y consumo de datos, es común trabajar \n",
    "con un lenguaje de programación como Python, en lugar de hacerlo directamente en Bash. \n",
    "En las siguientes preguntas, exploraremos la funcionalidad de Kafka utilizando Python con la biblioteca predeterminada, que **NO DEBES INSTALAR, ya que ya está disponible en la versión correcta**. \n",
    "Toda la documentación asociada con la API proporcionada la puedes encontrar en \n",
    "[Kafka](https://kafka-python.readthedocs.io/en/master/). Comenzaremos con lo básico que \n",
    "ya hemos cubierto: escribir en el tema de Kafka.\n",
    "Para esto, configuraremos [Kafka\n",
    "producer](https://kafka-python.readthedocs.io/en/master/apidoc/KafkaProducer.html)\n",
    "que insertará valores numéricos en un tema de Kafka cada 3 segundos. \n",
    "Mientras el productor escribe, procederemos a leer los mensajes en la pregunta 8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd264762",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "38bf942a78403a7ad24980a6b15ffa98",
     "grade": false,
     "grade_id": "cell-1ece0f1a8eb7b62c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Ejercicio 7: Escribe un tema en Kafka (0.5 puntos)\n",
    "\n",
    "\n",
    "Se te pide escribir una secuencia numérica de 300 números (del 1 al 300) \n",
    "en el tema de Kafka activity2<usuario> que acabamos de crear. Cada uno de \n",
    "los mensajes escritos en el tema debe contener información sobre el tema donde se escriben, \n",
    "una clave y el valor binario del número a escribir (por ejemplo, value=b'287'). \n",
    "Es esencial revisar la API asociada con el \n",
    "[Kafka producer](https://kafka-python.readthedocs.io/en/master/apidoc/KafkaProducer.html) \n",
    "para completar estos ejercicios.\n",
    "\n",
    "Para guiarte en el ejercicio, te proporcionamos una plantilla que puedes usar para completarlo. Debes completar las partes faltantes del código en la celda de código a continuación.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f43acf7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d58dd164d1266d805040690609048479",
     "grade": false,
     "grade_id": "cell-12dab48155c333c1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "import numpy as np\n",
    "<FILL_IN>\n",
    "for i in range(1,300):\n",
    "    <FILL_IN>\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eec72ac",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f66ccec2eedcafcdea59166358c92cd",
     "grade": true,
     "grade_id": "cell-17580c624ff2aa62",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb4be6b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "17f82f3ff94d83889c7064c2a5b16faf",
     "grade": false,
     "grade_id": "cell-73d5b980eb6f8e12",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Ejercicio 8: Leer un tema de Kafka (1 punto)\n",
    "\n",
    "Usando la biblioteca de Python para Kafka, \n",
    "[Kafka](https://kafka-python.readthedocs.io/en/master/),\n",
    "lee los mensajes enviados en el ejercicio 7, mostrando solo los valores, \n",
    "no las otras propiedades del mensaje. Es importante revisar el uso de Kafka en \n",
    "([Python](https://kafka-python.readthedocs.io/en/master/usage.html))\n",
    "y los parámetros del consumidor de \n",
    "[Kafka\n",
    "consumer](https://kafka-python.readthedocs.io/en/master/apidoc/KafkaConsumer.html).\n",
    "\n",
    "Como en el ejercicio anterior, aquí te proporcionamos una plantilla que puedes usar para completar el ejercicio. Debes completar las partes faltantes del código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c09cfdd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0f2b632680feba17e8414c875a9edbe",
     "grade": false,
     "grade_id": "cell-56844d262bc72fb4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "<FILL_IN>\n",
    "for message in consumer:\n",
    "    <FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001eb2ef",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51e344e77dd90c924bbc77e661c69763",
     "grade": true,
     "grade_id": "cell-093c5a1a7e6ef493",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85362c83",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dca9aa94591de52e6fd58977dcd536cc",
     "grade": false,
     "grade_id": "cell-1189e62b37db4192",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Part III: Procesamiento de datos en tiempo real con Apache Spark Streaming (6.5 puntos)\n",
    "\n",
    "En esta parte de la actividad, nos centraremos en el uso de Apache Spark para procesar datos en tiempo real. Aquí, utilizaremos:\n",
    "\n",
    "-   [Spark Streaming](https://downloads.apache.org/spark/docs/3.3.0/streaming-programming-guide.html):\n",
    "    Spark Streaming es un motor de procesamiento de flujos escalable y tolerante a fallos, \n",
    "    construido sobre Apache Spark. Permite el procesamiento de datos en tiempo real con alta capacidad de               rendimiento y baja latencia. Con Spark Streaming, puedes realizar análisis en tiempo real, \n",
    "    aprendizaje automático y procesamiento de grafos sobre datos en streaming.\n",
    "-   [Spark Structured Streaming](https://downloads.apache.org/spark/docs/3.3.0/structured-streaming-programming-guide.html):\n",
    "    Structured Streaming es una API de alto nivel para el procesamiento de flujos en Spark. Proporciona una             interfaz declarativa y similar a SQL para procesar datos de streaming estructurados.\n",
    "    Con Structured Streaming, puedes escribir consultas de streaming que se integran de manera \n",
    "    fluida con el procesamiento por lotes, lo que te permite construir pipelines de procesamiento \n",
    "    de datos de extremo a extremo.\n",
    "   \n",
    "\n",
    "En esta parte de la actividad, nos centraremos en la red social Mastodon para procesar sus datos en streaming. Es una buena idea familiarizarse con la estructura JSON de un \"toot\".(check the link [Mastodon\\'s API\n",
    "webpage](https://docs.joinmastodon.org/entities/Status/)), para facilitar la comprensión de los diferentes ejercicios que proponemos\n",
    "\n",
    "**Notas importantes:**\n",
    "\n",
    "-   Ten en cuenta que, al usar la versión 3.3.0 de Apache Spark, puede haber diferencias en la API y otras características en comparación con las versiones más recientes. Asegúrate de consultar la documentación específica para esa versión cuando necesites más detalles.\n",
    "\n",
    "-   Recuerda adjuntar una captura de pantalla del resultado de la ejecución de cada ejercicio en esta parte de la actividad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f34925",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0cd0bfad9927b476b44d8c4910e8766b",
     "grade": false,
     "grade_id": "cell-d370b874b6fb9f3d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Ejercicio 10: Spark Streaming (3.5 puntos)\n",
    "\n",
    "En este ejercicio, analizarás la actividad en Mastodon contando los toots en una ventana de tiempo. Contar es una de las operaciones fundamentales en Spark, y en esta actividad, aprovecharemos la biblioteca de Spark Streaming para mejorar nuestras capacidades de conteo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55a340a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d9b1414f014c083d745128ba030e14b",
     "grade": false,
     "grade_id": "cell-0268954ec18f3d70",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Ejercicio 10.1: Counting in Windows (0.5 puntos)\n",
    "\n",
    "Como sabrás, la biblioteca Spark Streaming **procesa los datos utilizando el concepto de time windows**, \n",
    "agrupando los elementos de datos según el tiempo en que fueron recibidos. Este enfoque \n",
    "permite el procesamiento por lotes de datos de streaming, habilitando análisis **sobre intervalos de tiempo distintos**. Verás que la sintaxis para realizar operaciones sobre los RDDs dentro de estas ventanas de tiempo es **prácticamente equivalente a las operaciones estándar de RDD** que ya conoces.\n",
    "\n",
    "Completa el código a continuación para obtener **el número de toots originales publicados cada cinco segundos**. Excluye los retweets de tu conteo. Puede que necesites consultar [Mastodon\n",
    "API](https://docs.joinmastodon.org/entities/Status/) para entender cómo están estructurados los toots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42ae935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "# Initialize SparkContext and StreamingContext with a 1-second batch interval\n",
    "app_name = <FILLIN>  # Name of your application\n",
    "\n",
    "# Create the SparkContext\n",
    "try:\n",
    "    sc = SparkContext(\"local[*]\", appName = app_name)\n",
    "except ValueError:\n",
    "    sc.stop()\n",
    "    sc = SparkContext(\"local[*]\", appName = app_name)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Create the StreamingContext\n",
    "batch_interval = 5  # Batch interval in seconds\n",
    "ssc = StreamingContext(<FILLIN>)\n",
    "ssc.checkpoint(\"checkpoint\")  # Necessary for updateStateByKey operation\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_server = <FILLIN>  # Kafka server address\n",
    "kafka_topic = <FILLIN>   # Kafka topic\n",
    "kafka_group = <FILLIN>   # Kafka consumer group, first surname of each member of the group separated by an underscore.\n",
    "\n",
    "kafkaParams = {\n",
    "    \"metadata.broker.list\": kafka_server,\n",
    "    \"group.id\": kafka_group\n",
    "}\n",
    "\n",
    "# Create a DStream that connects to Kafka\n",
    "kafkaStream = KafkaUtils.createDirectStream(<FILLIN>)\n",
    "\n",
    "# Count each toot as 1 and update the total count\n",
    "tootCounts = kafkaStream\\\n",
    "    .map(lambda x: json.loads(x[1]))\\\n",
    "    .filter(<FILLIN>)\\\n",
    "    .<FILLIN>\n",
    "    ...\n",
    "    <FILLIN>\n",
    "\n",
    "\n",
    "# Print the cumulative count\n",
    "tootCounts.pprint()\n",
    "\n",
    "# Start the computation\n",
    "try:\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    ssc.stop()\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0fe49f",
   "metadata": {},
   "source": [
    "Añade la captura de pantalla del output **aquí**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30120c9b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dffeea1fca741a1d4c7b248db66c4d89",
     "grade": false,
     "grade_id": "cell-9b4854b7aca5fd3a",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Ejercicio 10.2: Contando Toots por Idioma (0.5 puntos)\n",
    "\n",
    "Como observaste en el Ejercicio 2.1, el proceso es bastante similar a trabajar con RDDs. \n",
    "Ahora, vamos a profundizar en un análisis más complejo al **contar cuántos toots originales se crean \n",
    "por idioma cada 5 segundos**. Para mejorar la legibilidad, te solicitamos que ordenes \n",
    "los idiomas en orden descendente según el número de toots y limites la salida a los 10 principales idiomas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507f766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import json\n",
    "\n",
    "# Initialize SparkContext and StreamingContext with a 1-second batch interval\n",
    "app_name = \"FILLIN\"  # Name of your application\n",
    "\n",
    "# Create the SparkContext\n",
    "try:\n",
    "    sc = SparkContext(\"local[*]\", appName = app_name)\n",
    "except ValueError:\n",
    "    sc.stop()\n",
    "    sc = SparkContext(\"local[*]\", appName = app_name)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Create the StreamingContext\n",
    "batch_interval = 5  # Batch interval in seconds\n",
    "ssc = StreamingContext(<FILLIN>)\n",
    "ssc.checkpoint(\"checkpoint\")  # Necessary for updateStateByKey operation\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_server = <FILLIN>  # Kafka server address\n",
    "kafka_topic = <FILLIN>   # Kafka topic\n",
    "kafka_group = <FILLIN>   # Kafka consumer group, first surname of each member of the group separated by an underscore.\n",
    "\n",
    "kafkaParams = {\n",
    "    \"metadata.broker.list\": kafka_server,\n",
    "    \"group.id\": kafka_group\n",
    "}\n",
    "\n",
    "# Create a DStream that connects to Kafka\n",
    "kafkaStream = KafkaUtils.createDirectStream(<FILLIN>)\n",
    "\n",
    "# Count the number of toots per language\n",
    "tootLangCounts = kafkaStream\\\n",
    "    .<FILLIN>\n",
    "\n",
    "# Print the cumulative count\n",
    "tootLangCounts.pprint(<FILLIN>)\n",
    "\n",
    "# Start the computation\n",
    "try:\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    ssc.stop()\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66669363",
   "metadata": {},
   "source": [
    "Añade la captura de pantalla del output **aquí**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20739d91",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aefbb9a14b42387cbf90962af2a4daf9",
     "grade": false,
     "grade_id": "cell-567bdf2c648dc8c5",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Ejercicio 10.3: Manteniendo el Conteo (0.5 puntos)\n",
    "\n",
    "\n",
    "Hasta ahora, hemos estado obteniendo resultados específicos de lotes, lo cual es generalmente útil. \n",
    "Pero, ¿qué pasa si quieres obtener una visión más amplia, manteniendo la información a través de las \n",
    "ventanas para, por ejemplo, acumular tendencias a lo largo del tiempo? \n",
    "Ese es el enfoque de nuestra próxima exploración.\n",
    "\n",
    "En este ejercicio, te invitamos a modificar el script anterior para **mantener un conteo acumulado de todos los toots originales, categorizados por idioma**. En lugar de simplemente contar nuevos toots cada cinco segundos, vamos a **sumarlos continuamente**. Piensa en ello como un marcador que se actualiza constantemente con el número total de toots originales en cada idioma desde el momento en que comenzamos a hacer streaming.\n",
    "\n",
    "Para lograr esto, trabajaremos con las transformaciones \n",
    "[**stateful transformations in Spark Streaming**](https://downloads.apache.org/spark/docs/3.3.0/streaming-programming-guide.html#caching--persistence).\n",
    "Esto es una forma sofisticada de decir que recordaremos los datos pasados y los utilizaremos \n",
    "en nuestros cálculos actuales. Es similar a mantener un total acumulado en una variable \n",
    "global en lugar de comenzar desde cero cada vez.\n",
    "\n",
    "**Te invitamos a completar el siguiente script:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9b283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import json\n",
    "\n",
    "# Initialize SparkContext and StreamingContext with a 1-second batch interval\n",
    "app_name = <FILLIN>  # Name of your application\n",
    "\n",
    "# Create the SparkContext\n",
    "try:\n",
    "    sc = SparkContext(\"local[*]\", appName=\"app_name\")\n",
    "except ValueError:\n",
    "    sc.stop()\n",
    "    sc = SparkContext(\"local[*]\", appName=\"app_name\")\n",
    "\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "batch_interval = 5  # Batch interval in seconds\n",
    "ssc = StreamingContext(<FILLIN>)\n",
    "ssc.checkpoint(\"checkpoint\")  # Necessary for updateStateByKey operation\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_server = <FILLIN>  # Kafka server address\n",
    "kafka_topic = <FILLIN>   # Kafka topic\n",
    "kafka_group = <FILLIN>   # Kafka consumer group, first surname of each member of the group separated by an underscore.\n",
    "\n",
    "kafkaParams = {\n",
    "    \"metadata.broker.list\": kafka_server,\n",
    "    \"group.id\": kafka_group\n",
    "}\n",
    "\n",
    "\n",
    "# Create a DStream that connects to Kafka\n",
    "kafkaStream = KafkaUtils.createDirectStream(<FILLIN>)\n",
    "\n",
    "# Update the cumulative count using updateStateByKey\n",
    "def updateFunction(newValues, runningCount):\n",
    "    <FILLIN>\n",
    "    return <FILLIN>\n",
    "\n",
    "# Count each toot as 1 and update the total count\n",
    "tootCounts = kafkaStream\\\n",
    "    .map(lambda x: json.loads(x[1]))\\\n",
    "    .<FILLIN>\n",
    "    ....\n",
    "    .<FILLIN>\n",
    "    .updateStateByKey(<FILLIN>)\\\n",
    "    <FILLIN>\n",
    "\n",
    "# Print the cumulative count\n",
    "tootCounts.pprint()\n",
    "\n",
    "# Start the computation\n",
    "try:\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    ssc.stop()\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8d9158",
   "metadata": {},
   "source": [
    "Añade la captura de pantalla del output **aquí**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03fe035",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1bd12b46396dba5480f120d546600192",
     "grade": false,
     "grade_id": "cell-b4f488f79d86b434",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Ejercicio 10.4: Windowed Counting (1 punto)\n",
    "\n",
    "Como has observado, Spark Streaming es increíblemente flexible y \n",
    "fácil de usar, y aquí tienes un truco interesante que puede realizar: \n",
    "**te permite encontrar un punto intermedio entre contar toots [time\n",
    "window](https://downloads.apache.org/spark/docs/3.3.0/streaming-programming-guide.html#window-operations)\n",
    "y mantener un total acumulado**. Imaginemos que queremos crear un panel de control, \n",
    "como un tablero, que muestre el número de toots realizados en cada idioma. \n",
    "El giro es que **queremos que esta actualización ocurra cada 5 segundos, \n",
    "pero estamos rastreando los conteos durante un minuto completo**.\n",
    "\n",
    "Así, cada 5 segundos, nuestro panel se actualiza, proporcionándonos \n",
    "el último conteo acumulado durante un minuto. Es como tener un marcador en vivo \n",
    "que se actualiza con frecuencia y también realiza un seguimiento de lo que ha ocurrido \n",
    "en los últimos 60 segundos, no solo en los últimos 5. De esta forma, obtienes \n",
    "tanto actualizaciones inmediatas como una vista más amplia de lo que está ocurriendo, \n",
    "todo al mismo tiempo. Muestra solo los 10 principales idiomas.\n",
    "\n",
    "**Modifica el siguiente script para lograr este objetivo:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0ff66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import json\n",
    "\n",
    "# Initialize SparkContext and StreamingContext with a 1-second batch interval\n",
    "app_name = \"FILLIN\"  # Name of your application\n",
    "\n",
    "# Create the SparkContext\n",
    "try:\n",
    "    sc = SparkContext(\"local[*]\", appName=\"app_name\")\n",
    "except ValueError:\n",
    "    sc.stop()\n",
    "    sc = SparkContext(\"local[*]\", appName=\"app_name\")\n",
    "\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "ssc = StreamingContext(<FILLIN>)\n",
    "ssc.checkpoint(\"checkpoint\")  # Necessary for updateStateByKey operation\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_server = <FILLIN>  # Kafka server address\n",
    "kafka_topic = <FILLIN>   # Kafka topic\n",
    "kafka_group = <FILLIN>         # Kafka consumer group, first surname of each member of the group separated by an underscore.\n",
    "\n",
    "kafkaParams = {\n",
    "    \"metadata.broker.list\": kafka_server,\n",
    "    \"group.id\": kafka_group\n",
    "}\n",
    "\n",
    "\n",
    "# Create a DStream that connects to Kafka\n",
    "kafkaStream = KafkaUtils.createDirectStream(<FILLIN> )\n",
    "\n",
    "# Count each toot as 1 and update the total count. Use a 60-second window with a 5-second slide\n",
    "tootCounts = kafkaStream\\\n",
    "    .<FILLIN>\n",
    "\n",
    "# Print the cumulative count\n",
    "tootCounts.pprint()\n",
    "\n",
    "# Start the computation\n",
    "try:\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    ssc.stop()\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03514a4b",
   "metadata": {},
   "source": [
    "Añade la captura de pantalla del output **aquí**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e81f73",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cc5848aaa0749e4d1c219b09910479c9",
     "grade": false,
     "grade_id": "cell-8a76c6c76b006e10",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Ejercicio 10.5: Powering Up (1 punto)\n",
    "\n",
    "\n",
    "Muy bien, ya sabemos que los RDDs en Spark son increíblemente versátiles: \n",
    "son como la navaja suiza del procesamiento de datos de stream. Puedes hacer prácticamente \n",
    "cualquier operación con ellos. Sin embargo, **a medida que las cosas se vuelven más complejas, \n",
    "el desafío aumenta.**\n",
    "\n",
    "Ahora, vamos a hacer nuestro panel de control aún más interesante. En lugar de solo mostrar \n",
    "cuántos toots tenemos por minuto, agreguemos algunas características nuevas. \n",
    "¿No sería interesante **rastrear la longitud promedio de estos toots?**\n",
    "Y hay más: vamos a descubrir **quién es el usuario más seguido entre los que han tooteado en ese minuto.**\n",
    "\n",
    "Espera, **¡hay más!** Para hacer toda esta información súper fácil de leer y entender, vamos a presentarla en un formato de tabla ordenado y limpio. No se trata solo de los datos, sino de hacerlos amigables para el usuario y visualmente digeribles.\n",
    "\n",
    "La tabla resultante debe actualizarse en intervalos de 5 segundos, y las ventanas de promedio deben ser de 60 segundos. Las columnas de esa tabla deben ser:\n",
    "    \n",
    "-   **`lang`:** Idioma\n",
    "-   **`num_toots`:** Número de toots originales en ese idioma\n",
    "-   **`avg_len_content`:** Longitud promedio (en caracteres) del contenido del toot\n",
    "-   **`user`:** Usuario más seguido\n",
    "-   **`followers`:** Número de seguidores de ese usuario\n",
    "\n",
    "Para hacer que la salida sea más legible, limita el número de filas a 10.\n",
    "\n",
    "**TIP:** Hay un ejemplo muy útil en [Spark Streaming](https://downloads.apache.org/spark/docs/3.3.0/streaming-programming-guide.html#dataframe-and-sql-operations). ¡Búscalo!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c769f1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1da10c",
   "metadata": {},
   "source": [
    "Añade la captura de pantalla del output **aquí**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5d3514",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5efc95bda09ee23e35d94a6bc45683c1",
     "grade": false,
     "grade_id": "cell-78a4d1f04421041f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Ejercicio 11: Structured Streaming (3 puntos)\n",
    "\n",
    "Como has visto en el último ejercicio, dependiendo de las operaciones, \n",
    "la API de Spark Streaming puede no resultar tan conveniente, \n",
    "especialmente porque tienes que tratar con APIs de bajo nivel. Afortunadamente, **Spark provides a\n",
    "high-level API called [Spark Structured\n",
    "Streaming](https://downloads.apache.org/spark/docs/3.3.0/structured-streaming-programming-guide.html)** que te permite **expresar los cálculos de streaming de la misma manera que expresarías un cálculo por lotes sobre datos estructurados estáticos**, como los que usarías en el procesamiento por lotes.\n",
    "\n",
    "En este conjunto de ejercicios, te sumergirás en el fascinante mundo de Spark Structured Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d938ae",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff78c173ac93aefd8ed84e53ebf1f6ac",
     "grade": false,
     "grade_id": "cell-bb18ad1646897ea8",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Ejercicio 11.1: Getting the Schema (0.5 puntos)\n",
    "\n",
    "Una de las características más interesantes de Spark Structured Streaming es \n",
    "**cómo maneja los datos estructurados**. Por ejemplo, el flujo de datos en nuestro tema de Kafka, \n",
    "donde **cada \"toot\" viene en un formato JSON ordenado**, lo que lo hace estructurado y organizado.\n",
    "\n",
    "De manera similar a cómo se trabaja con DataFrames en Spark, **Structured Streaming usa esquemas de datos para analizar datos estructurados**, esencialmente un plano de cómo están dispuestos los datos. Para el procesamiento por lotes, Spark puede a menudo deducir esta estructura directamente de los datos. Sin embargo, con los datos de streaming, es un poco diferente: **necesitamos definir esta estructura de antemano.**\n",
    "\n",
    "En los siguientes ejercicios, utilizaremos un truco conveniente: **en lugar de definir manualmente**\n",
    "la estructura compleja de un \"toot\", inicialmente **extraeremos algunos toots de Kafka y los analizaremos en un lote para aprender su esquema**. Es como echar un vistazo para entender cómo están organizadas las cosas. \n",
    "Una vez que tengamos el esquema, lo aplicaremos a nuestra pipeline de streaming.\n",
    "\n",
    "Tu tarea en este ejercicio es realizar esta transformación. Luego, utilizando las operaciones de DataFrame con las que ya estás familiarizado, crearás una tabla con las siguientes columnas que nos permitirá ver los toots individualmente a medida que se procesan:\n",
    "\n",
    "\n",
    "-   **`id`:** Identificador único para cada toot\n",
    "-   **`created_at`:** Fecha y hora en que se creó el toot\n",
    "-   **`content`:** El contenido del toot\n",
    "-   **`language`:** El idioma del toot\n",
    "-   **`username`:** El nombre de usuario del autor del toot\n",
    "-   **`followers_count`:** Número de seguidores del autor.\n",
    "\n",
    "\n",
    "Recuerda que nos **interesan los toots originales**. Filtra aquellos que son retweets.\n",
    "\n",
    "Otro aspecto fundamental que debes gestionar aquí es **seleccionar el outputMode adecuado**. Consulta la \n",
    "[the documentation](https://downloads.apache.org/spark/docs/3.3.0/structured-streaming-programming-guide.html#output-modes)\n",
    "y elige el que mejor se adapte a este ejercicio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fa6093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.functions import json_tuple, from_json, col\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Initialize Spark Session for Structured Streaming\n",
    "app_name = \"activity3_1\" + <FILLIN> # Replace with your Spark app name must include the username of the members of the group\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(app_name) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_topic = <FILLIN>\n",
    "kafka_bootstrap_servers = <FILLIN>  # Replace with your Kafka bootstrap servers\n",
    "\n",
    "# Read a small batch of data from Kafka for schema inference!\n",
    "batch_df = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"{\\\"\" + kafka_topic + \"\\\":{\\\"0\\\":10}}\") \\\n",
    "    .load()\n",
    "\n",
    "# Infer schema\n",
    "schema = spark.read.json(batch_df.selectExpr(\"CAST(value AS STRING)\").rdd.map(lambda x: x[0])).schema\n",
    "\n",
    "# Create streaming DataFrame by reading data from Kafka\n",
    "toots = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", <FILLIN>) \\\n",
    "    .load()\n",
    "\n",
    "# Parse the value column as JSON and apply the infered schema. Then select the columns we need.\n",
    "toots_df = toots\\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"parsed_value\"))\\\n",
    "    .select(<FILLIN>)\n",
    "\n",
    "try:\n",
    "    # Open stream to console (you need to execute it in a terminal to see the output)\n",
    "    query = toots_df \\\n",
    "            .writeStream \\\n",
    "            .outputMode(<FILLIN>) \\\n",
    "            .format(\"console\")\\\n",
    "            .start()\n",
    "\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    query.stop()\n",
    "    spark.stop()\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b639ae",
   "metadata": {},
   "source": [
    "Añade la captura de pantalla del output **aquí**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58af1b1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ae1177102a8519ccc35de3b5e17da52e",
     "grade": false,
     "grade_id": "cell-6646f36d2cf2c16b",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Ejercicio 11.2: Agregando Datos desde un Flujo (0.5 puntos)\n",
    "\n",
    "Spark Structured Streaming es realmente poderoso, especialmente cuando se realizan operaciones sobre un flujo continuo de datos. En este ejercicio, profundizaremos en Structured Spark Streaming, enfocándonos específicamente en la agregación de datos desde un flujo de Kafka. Es similar a lo que hicimos en el Ejercicio 2. Tu misión es **contar el número de toots originales en cada idioma.**\n",
    "\n",
    "Aquí te mostramos cómo debe ser tu salida:\n",
    "\n",
    "-   **`language`:** Esta columna muestra el idioma de los toots.\n",
    "-   **`count`:** Aquí es donde mostrarás el número de toots por cada idioma.\n",
    "\n",
    "Tu tabla debe **acumular estos conteos cada 10 segundos, y también debe seguir acumulándolos**. Además, hazla  amigable para el usuario **ordenando los idiomas por el número de toots, con los idiomas más conversadores en la parte superior.**\n",
    "\n",
    "Ahora, aquí hay una **parte clave** de este ejercicio: **necesitas elegir el modo de salida adecuado para tu consulta de streaming**. Recuerda, el modo de **salida determina cómo se escribe cada lote de datos resultante en el destino de salida**. Tus opciones son los modos 'Complete', 'Append' y 'Update'. Piensa cuál encaja mejor para nuestro escenario de conteo acumulativo y ordenado. Y **no olvides escribir tu razonamiento en los comentarios.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ac3700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.functions import from_json, col, window\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Initialize Spark Session for Structured Streaming\n",
    "app_name = \"activity3_2\" + <FILLIN> # Replace with your Spark app name must include the username of the members of the group\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(app_name) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_topic = <FILLIN>\n",
    "kafka_bootstrap_servers = <FILLIN>  # Replace with your Kafka bootstrap servers\n",
    "\n",
    "# Read a small batch of data from Kafka for schema inference!\n",
    "batch_df = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"{\\\"\" + kafka_topic + \"\\\":{\\\"0\\\":10}}\") \\\n",
    "    .load()\n",
    "\n",
    "# Infer schema\n",
    "schema = spark.read.json(batch_df.selectExpr(\"CAST(value AS STRING)\").rdd.map(lambda x: x[0])).schema\n",
    "\n",
    "# Create streaming DataFrame by reading data from Kafka\n",
    "toots = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", <FILLIN>) \\\n",
    "    .load()\n",
    "\n",
    "# Parse the value column as JSON and apply the infered schema. Then select the columns we need.\n",
    "toots_df = toots\\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"parsed_value\"))\\\n",
    "    .filter(<FILLIN>)\\\n",
    "    .select(<FILLIN>)\\\n",
    "    .groupBy(<FILLIN>)\\\n",
    "    .<FILLIN>\n",
    "    ...\n",
    "    <FILLIN>\n",
    "\n",
    "\n",
    "try:\n",
    "    # Open stream to console (you need to execute it in a terminal to see the output)\n",
    "    query = toots_df \\\n",
    "            .writeStream \\\n",
    "            .outputMode(<FILLIN>) \\\n",
    "            .format(\"console\")\\\n",
    "            .<FILLIN>\n",
    "            ...\n",
    "            .<FILLIN>\n",
    "            .trigger(<FILLIN>)\\\n",
    "            .start()\\\n",
    "\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    query.stop()\n",
    "    spark.stop()\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e862fe",
   "metadata": {},
   "source": [
    "Añade la captura de pantalla del output **aquí**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cf8519",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2635978bab496b500adbd50d19b0ad17",
     "grade": false,
     "grade_id": "cell-42a4a46bcc6a639c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Ejercicio 11.3: Windowed Counting (1 punto)\n",
    "\n",
    "¡Buen trabajo! Has aprendido a realizar agregaciones y a hacer un seguimiento de los conteos a lo largo del tiempo. Como notaste en el Ejercicio 2.4, a veces es más efectivo mantener estos conteos en **specific time windows**. Ahora, queremos que apliques esta técnica utilizando [functions available in Spark Structured\n",
    "Streaming](https://downloads.apache.org/spark/docs/3.3.0/structured-streaming-programming-guide.html#window-operations-on-event-time). \n",
    "Ten en cuenta que Spark Structured Streaming maneja el tiempo de manera diferente a Spark Streaming, por lo que deberás considerar esto al analizar e interpretar los resultados.\n",
    "\n",
    "Tu tarea es **crear una tabla que muestre un conteo de la cantidad de toots originales (recuerda filtrar los retweets) realizados en cada idioma, segmentados dentro de un marco de tiempo específico.** Para este ejercicio, debes usar una sliding window de un minuto, con los datos refrescándose cada 5 segundos. Este enfoque te permitirá monitorear de cerca la frecuencia de los toots en diferentes idiomas a lo largo de intervalos breves y superpuestos.\n",
    "\n",
    "Te pedimos que proporciones una tabla con la siguiente estructura:\n",
    "\n",
    "- **`window`:** Muestra el rango de tiempo\n",
    "- **`language`:** Esta columna muestra el idioma de los toots.\n",
    "- **`count`:** Aquí es donde mostrarás el número de toots para cada idioma.\n",
    "\n",
    "Los resultados deben **ordenarse por time window y conteo en orden descendente.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6adaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.functions import json_tuple, from_json, col, window\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Initialize Spark Session for Structured Streaming\n",
    "app_name = \"activity3_3\" + <FILLIN> # Replace with your Spark app name must include the username of the members of the group\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(app_name) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_topic = <FILLIN>\n",
    "kafka_bootstrap_servers = <FILLIN>  # Replace with your Kafka bootstrap servers\n",
    "\n",
    "# Read a small batch of data from Kafka for schema inference!\n",
    "batch_df = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"{\\\"\" + kafka_topic + \"\\\":{\\\"0\\\":10}}\") \\\n",
    "    .load()\n",
    "\n",
    "# Infer schema\n",
    "schema = spark.read.json(batch_df.selectExpr(\"CAST(value AS STRING)\").rdd.map(lambda x: x[0])).schema\n",
    "\n",
    "# Create streaming DataFrame by reading data from Kafka\n",
    "toots = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", <FILLIN>) \\\n",
    "    .load()\n",
    "\n",
    "# Parse the value column as JSON and apply the infered schema. Then select the columns we need.\n",
    "toots_df = toots\\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"parsed_value\"))\\\n",
    "    .filter(<FILLIN>)\\\n",
    "    .select(<FILLIN>)\\\n",
    "    .groupBy(\n",
    "        window(<FILLIN>),\n",
    "        <FILLIN>\n",
    "    )\\\n",
    "    .count()\\\n",
    "    .orderBy(<FILLIN>, <FILLIN>, ascending=False)\n",
    "\n",
    "try:\n",
    "    # Open stream to console (you need to execute it in a terminal to see the output)\n",
    "    query = toots_df \\\n",
    "            .writeStream \\\n",
    "            .outputMode(<FILLIN>) \\\n",
    "            .format(\"console\")\\\n",
    "            .option(\"truncate\", \"false\")\\\n",
    "            .trigger(<FILLIN>)\\\n",
    "            .start()\\\n",
    "\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    query.stop()\n",
    "    spark.stop()\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040630ff",
   "metadata": {},
   "source": [
    "Añade la captura de pantalla del output **aquí**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f9273a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "35396805d1fa1d4d6c64113df0e936b4",
     "grade": false,
     "grade_id": "cell-14149db32b69fa0d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Ejercicio 11.4: Unir Flujos (1 punto)\n",
    "\n",
    "En este último ejercicio, exploraremos una característica muy interesante de Spark Streaming que te permite unir \n",
    "**[unir dos streams](https://downloads.apache.org/spark/docs/3.3.0/structured-streaming-programming-guide.html#stream-stream-joins)\n",
    "y analizarlos!**\n",
    "\n",
    "\n",
    "Para simplificar las cosas, ya te proporcionamos dos flujos de datos pre-agregados. \n",
    "El primero, que se encuentra en el tema de Kafka **`mastodon_toots_original_domain`**, muestra el conteo de toots originales para varias instancias de Mastodon (recuerda que Mastodon es una federación de instancias) durante **fixed one-minute window**. \n",
    "El segundo flujo, en el **`mastodon_toots_retoot_domain` topic**,, presenta datos similares, pero para los toots que son retweets (compartidos) de otros toots. Los datos almacenados en los temas de Kafka tienen la misma estructura en formato JSON:\n",
    "\n",
    "- Una `window` estructura con dos elementos `string` type: `start` and `end` \n",
    "- Un componente `string` llamado `mastodon_instance` con el topic.\n",
    "- Un elemento de tipo `integer` llamado `count` con el número de toots ealizados en ese dominio en el rango de tiempo específico.\n",
    "\n",
    "Dado que la estructura de los datos es bastante sencilla, esta vez **te pedimos que la definas manualmente**, en lugar de usar el truco de batch. Una vez que hayas configurado las estructuras, **deberás abrir un flujo para cada fuente de Kafka.** El siguiente paso es unir estos flujos. Queremos que realices una **left join del flujo de toots originales con el flujo de retweets**. Después de completar la unión, tu salida debe incluir:\n",
    "\n",
    "\n",
    "- **`window`:** el rango de tiempo\n",
    "- **`mastodon_instance`:** el dominio de la instancia de Mastodon\n",
    "- **`original_count`:** número de toots originales publicados en ese dominio durante ese rango de tiempo\n",
    "- **`retweet_count`:** número de toots de retweets publicados en ese dominio durante ese rango de tiempo\n",
    "\n",
    "**TIP**: Realizar una unión en línea de dos flujos no es una tarea fácil, y hay muchas restricciones que debes respetar. \n",
    "Lee cuidadosamente [la documentación](https://downloads.apache.org/spark/docs/3.3.0/structured-streaming-programming-guide.html#stream-stream-joins)\n",
    "y recuerda que estamos usando la versión 2.4.0. \n",
    "Además, recuerda que estamos realizando **join over time**, y este es un componente **clave**. \n",
    "Conceptos como los que has aprendido sobre windows son fundamentales junto con conceptos como [watermarking](https://downloads.apache.org/spark/docs/3.3.0/structured-streaming-programming-guide.html#stream-stream-joins).\n",
    "Y recuerda, **los modos de salida son complicados** y debes elegir uno que sea adecuado para el trabajo que deseas hacer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b59e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructType, StructField, IntegerType\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.functions import from_json, col, window, to_timestamp, struct\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Context\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Initialize Spark Session for Structured Streaming\n",
    "app_name = \"activity3_4\" + <FILLIN> # Replace with your Spark app name must include the username of the members of the group\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(app_name) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schema for the incoming data\n",
    "schema = StructType(<FILLIN>)\n",
    "\n",
    "# Define Kafka parameters\n",
    "toots_original_topic = <FILLIN>\n",
    "toots_retoot_topic = <FILLIN>\n",
    "kafka_bootstrap_servers = <FILLIN>  # Replace with your Kafka bootstrap servers\n",
    "\n",
    "# Create streaming DataFrame by reading original toots data from Kafka\n",
    "toots_original = spark \\\n",
    "    .readStream \\\n",
    "    .<FILLIN>\n",
    "    ...\n",
    "    .<FILLIN>\n",
    "    .load()\n",
    "\n",
    "# Parse the value column as JSON and apply the infered schema. Then select the columns we need.\n",
    "toots_original_df = toots_original\\\n",
    "    .<FILLIN>\n",
    "    ...\n",
    "    .<FILLIN>\n",
    "\n",
    "# Create streaming DataFrame by reading retoots data from Kafka\n",
    "toots_retoot = spark \\\n",
    "    .readStream \\\n",
    "    .<FILLIN>\n",
    "    ...\n",
    "    .<FILLIN>\n",
    "    .load()\n",
    "\n",
    "# Parse the value column as JSON and apply the infered schema. Then select the columns we need.\n",
    "toots_retoot_df = toots_retoot\\\n",
    "    .<FILLIN>\n",
    "    ...\n",
    "    .<FILLIN>\n",
    "\n",
    "# Join the two streams\n",
    "toots_join_df = toots_original_df.join(<FILLIN>...<FILLIN>)\n",
    "\n",
    "try:\n",
    "    # Start running the query that prints the running counts to the console\n",
    "    query = toots_join_df\\\n",
    "            .writeStream \\\n",
    "            <FILLIN>\n",
    "            ...\n",
    "            <FILLIN>\n",
    "            .option(\"numRows\", 100)\\\n",
    "            .start()\\\n",
    "\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    query.stop()\n",
    "    spark.stop()\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4241decc",
   "metadata": {},
   "source": [
    "Añade la captura de pantalla del output **aquí**"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
